---
title: "Untitled"
output: html_document
date: '2022-11-05'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(tidymodels)
library(scales)
library(textrecipes)
library(finetune)
library(vip)
library(SHAPforxgboost)

theme_set(theme_light())


```

```{r}
ratings <- read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-25/ratings.csv")
details <- read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-25/details.csv")

```


```{r}
ratings_joined <-
  ratings %>%
  left_join(details, by = "id")

ggplot(ratings_joined, aes(average)) +
  geom_histogram(alpha = 0.8)
```

```{r}
ratings_joined %>% 
  filter(!is.na(minage)) %>% 
  mutate(minage = cut_number(minage, 4)) %>%
  ggplot(aes(x = minage, y = average, 
             fill = minage)) + 
  geom_boxplot(alpha = .5, 
               show.legend = FALSE)
```

### Tuning an xgboost model 

```{r}
game_split <- ratings_joined %>% 
  select(name, 
         average, 
         matches("min|max"), 
         boardgamecategory) %>% 
  na.omit() %>% 
  initial_split(strata = average)

train <- training(game_split)
test <-  testing(game_split)

set.seed(234)

game_folds <- vfold_cv(train, 
                       strata = average)
```

#### Using recipes for preprocessing 

```{r}

split_category <- function(x) {
  x %>% 
    str_split(", ") %>% 
    map(str_remove_all, "[:punct:]") %>% 
    map(str_squish) %>% 
    map(str_to_lower) %>% 
    map(str_replace_all, " ", "_") 
}

game_rec <- recipe(average ~ ., 
       data = train) %>% 
  update_role(name, new_role = "id") %>% 
  step_tokenize(boardgamecategory, 
                custom_token = split_category) %>% 
  step_tokenfilter(boardgamecategory, max_tokens = 30) %>% 
  step_tf(boardgamecategory)


# The following two steps are just there to check the result of the recipe 
game_prep <- prep(game_rec)

bake(game_prep, new_data = NULL)
```


### Model specification 

```{r}
xgb_spec <-  
  boost_tree(
    trees = tune(), 
    mtry = tune(), 
    min_n = tune(), 
    learn_rate = .01
  ) %>% 
  set_engine("xgboost") %>% 
  set_mode("regression")

```

```{r}
xgb_wf <- workflow(game_rec, xgb_spec)
```

```{r}
set.seed(234)

xgb_game_res <- tune_race_anova(
  # It takes a workflow as the first argument 
  xgb_wf,
  # Then it takes the folds
  resamples = game_folds,
  grid = 20, 
  control = control_race(verbose_elim = TRUE)
)

```


### Evaluating the model 

```{r}
plot_race(xgb_game_res)

show_best(xgb_game_res)
```

### Finalising the model 

```{r}

xgb_last <- xgb_wf %>% 
  # This selects the best performing model
  finalize_workflow(select_best(xgb_game_res, "rmse")) %>% 
  # This argument takes the split 
  # This is the only time that the testing data is used 
  # Which means that when you call collect_metrics(), 
  # the results are the final results from the testing set
  last_fit(game_split)

xgb_last %>% collect_metrics()
```

### Model explainabilty

```{r}
# Extracts the workflow
extract_workflow(xgb_last)

# Extracts the model 
xgb_fit <- extract_fit_parsnip(xgb_last)
```

The VIP approach uses the structure of the model 

```{r}
vip(xgb_fit, 
    geom = "point", 
    num_features = 12)
```

Shapley values are much more involved. 
Shapley additive explanations
For the sets of predictors that I have, if I arrange them in different coalitions, do they move the prediction up or down? What can we learn about the behaviour of our model, we can understand what it's like.
This is important becasue xg boost is one of those black box models that needs us to use interpretabilty algorithms on them.

```{r}
game_shap <- shap.prep(
  # Pulling out the underlying xg boost model
  xgb_model = extract_fit_engine(xgb_fit), 
  X_train = bake(game_prep,
                 has_role("predictor"),
                 new_data = NULL,
                 composition = "matrix")
)

```

The XGboost model is able to make use of the complex relationships shown below. 

```{r}
shap.plot.summary(game_shap)
```

XGBoost is also very good for modelling non-linear behaviours. 

```{r}
shap.plot.dependence(
  game_shap, 
  x = "minage", 
  color_feature = "minplayers", 
  size0 = 1.2, 
  smooth = FALSE, 
  add_hist = TRUE)
```








