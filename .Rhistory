ggplot(aes(x = minage, y = average,
fill = minage)) +
geom_boxplot(alpha = .5,
show.legend = FALSE)
game_split <- ratings_joined %>%
select(name,
average,
matches("min|max"),
boardgamecategory) %>%
na.omit() %>%
initial_split(strata = average)
train <- training(game_split)
test <-  testing(game_split)
set.seed(234)
game_folds <- vfold_cv(game_train,
strata = average)
game_folds <- vfold_cv(train,
strata = average)
library(textrecipes)
game_rec <- recipe(average ~ .,
data = train) %>%
update_role(name, new_role = "id") %>%
step_tokenize(boardgamecategory) %>%
step_tokenfilter(boardgamecategory, max_tokens = 30) %>%
step_tf(boardgamecategory)
game_prep <- prep(game_rec)
bake(game_prep, new_data = NULL)
split_category <- function(x) {
x %>%
str_split(", ") %>%
map(str_remove_all, "[:punct:]") %>%
map(str_to_lower) %>%
map(str_replace_all, " ", "_")
}
split_category <- function(x) {
x %>%
str_split(", ") %>%
map(str_remove_all, "[:punct:]") %>%
mpa(str_squish) %>%
map(str_to_lower) %>%
map(str_replace_all, " ", "_")
}
xgb_spec <-
boost_tree(
tree = tune(),
mtry = tune(),
min_n = tune(),
learn_rate = .01
) %>%
set_engine("xgboost") %>%
set_mode("regression")
xgb_spec <-
boost_tree(
trees = tune(),
mtry = tune(),
min_n = tune(),
learn_rate = .01
) %>%
set_engine("xgboost") %>%
set_mode("regression")
xgb_wf <- workflow(game_rec, xgb_spec)
install.packages("finetune")
install.packages("finetune")
install.packages("finetune")
library(finetune)
install.packages("finetune")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidymodels)
library(scales)
library(textrecipes)
library(finetune)
theme_set(theme_light())
ratings <- read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-25/ratings.csv")
details <- read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-25/details.csv")
ratings_joined <-
ratings %>%
left_join(details, by = "id")
ggplot(ratings_joined, aes(average)) +
geom_histogram(alpha = 0.8)
game_split <- ratings_joined %>%
select(name,
average,
matches("min|max"),
boardgamecategory) %>%
na.omit() %>%
initial_split(strata = average)
train <- training(game_split)
test <-  testing(game_split)
set.seed(234)
game_folds <- vfold_cv(train,
strata = average)
split_category <- function(x) {
x %>%
str_split(", ") %>%
map(str_remove_all, "[:punct:]") %>%
mpa(str_squish) %>%
map(str_to_lower) %>%
map(str_replace_all, " ", "_")
}
game_rec <- recipe(average ~ .,
data = train) %>%
update_role(name, new_role = "id") %>%
step_tokenize(boardgamecategory,
custom_token = split_category) %>%
step_tokenfilter(boardgamecategory, max_tokens = 30) %>%
step_tf(boardgamecategory)
xgb_spec <-
boost_tree(
trees = tune(),
mtry = tune(),
min_n = tune(),
learn_rate = .01
) %>%
set_engine("xgboost") %>%
set_mode("regression")
xgb_wf <- workflow(game_rec, xgb_spec)
set.seed(234)
?tune_race_anova
xgb_game_res <- tune_race_anova(
# It takes a workflow as the first argument
xg_wf,
# Then it takes the folds
resamples = game_folds,
grid = 20,
control = control_race(verbose_elim = TRUE)
)
xgb_game_res <- tune_race_anova(
# It takes a workflow as the first argument
xgb_wf,
# Then it takes the folds
resamples = game_folds,
grid = 20,
control = control_race(verbose_elim = TRUE)
)
split_category <- function(x) {
x %>%
str_split(", ") %>%
map(str_remove_all, "[:punct:]") %>%
map(str_squish) %>%
map(str_to_lower) %>%
map(str_replace_all, " ", "_")
}
game_rec <- recipe(average ~ .,
data = train) %>%
update_role(name, new_role = "id") %>%
step_tokenize(boardgamecategory,
custom_token = split_category) %>%
step_tokenfilter(boardgamecategory, max_tokens = 30) %>%
step_tf(boardgamecategory)
xgb_spec <-
boost_tree(
trees = tune(),
mtry = tune(),
min_n = tune(),
learn_rate = .01
) %>%
set_engine("xgboost") %>%
set_mode("regression")
set.seed(234)
xgb_game_res <- tune_race_anova(
# It takes a workflow as the first argument
xgb_wf,
# Then it takes the folds
resamples = game_folds,
grid = 20,
control = control_race(verbose_elim = TRUE)
)
split_category <- function(x) {
x %>%
str_split(", ") %>%
map(str_remove_all, "[:punct:]") %>%
map(str_squish) %>%
map(str_to_lower) %>%
map(str_replace_all, " ", "_")
}
game_rec <- recipe(average ~ .,
data = train) %>%
update_role(name, new_role = "id") %>%
step_tokenize(boardgamecategory,
custom_token = split_category) %>%
step_tokenfilter(boardgamecategory, max_tokens = 30) %>%
step_tf(boardgamecategory)
xgb_spec <-
boost_tree(
trees = tune(),
mtry = tune(),
min_n = tune(),
learn_rate = .01
) %>%
set_engine("xgboost") %>%
set_mode("regression")
xgb_wf <- workflow(game_rec, xgb_spec)
xgb_game_res <- tune_race_anova(
# It takes a workflow as the first argument
xgb_wf,
# Then it takes the folds
resamples = game_folds,
grid = 20,
control = control_race(verbose_elim = TRUE)
)
plot_race(xgb_game_res)
show_best(xgb_game_res)
xgb_last <- xgb_wf %>%
# This selects the best performing model
finalize_workflow(select_best(xgb_game_res, "rmse")) %>%
# This argument takes the split
# This is the only time that the testing data is used
# Which means that when you call collect_metrics(),
# the results are the final results from the testing set
last_fit(game_split)
xgb_last %>% collect_metrics()
library(vip)
extract_workflow(xgb_last)
# Extracts the model
extract_fit_parsnip(xgb_last)
# Extracts the model
xgb_fit <- extract_fit_parsnip(xgb_last)
vip(xgb_fit,
geom = "point",
num_features = 12)
install.packages("SHAPforxgboost")
library(SHAPforxgboost)
# The following two steps are just there to check the result of the recipe
game_prep <- prep(game_rec)
game_shap <- shap.prep(
# Pulling out the underlying xg boost model
xgb_model = extract_fit_engine(xgb_fit),
X_train = bake(game_prep,
has_role("predictor"),
new_data = NULL,
composition = "matrix")
)
shap.plot.summary(game_shap)
shap.plot.dependence(
game_shap,
x = "minage")
shap.plot.dependence(
game_shap,
x = "maxplaytime")
shap.plot.dependence(
game_shap,
x = "minage",
smooth = FALSE,
add_hist = TRUE)
shap.plot.dependence(
game_shap,
x = "minage",
color_feature = "minplayers",
smooth = FALSE,
add_hist = TRUE)
shap.plot.dependence(
game_shap,
x = "minage",
color_feature = "minplayers",
size0 = 1.2,
smooth = FALSE,
add_hist = TRUE)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(scales)
theme_set(theme_light())
vb_matches <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-19/vb_matches.csv', guess_max = 76000)
glimpse(vb_matches)
w_
vb_parsed <- vb_matches %>%
transmute(
circuit,
gender,
year,
w_attacks = w_p1_tot_attacks + w_p2_tot_attacks,
w_kills = w_p1_tot_kills + w_p2_tot_kills,
w_errors = w_p1_tot_errors + w_p1_tot_errors,
w_aces = w_p1_tot_aces + w_p2_tot_aces,
w_blocks = w_p1_tot_blocks + w_p2_tot_blocks,
w_serrve_errors = w_p1_tot_serve_errors + w_p2_tot_serve_errors,
w_blocks = w_p1_tot_blocks + w_p2_tot_blocks,
w_digs = w_p1_tot_digs + w_p2_tot_digs,
l_attacks = l_p1_tot_attacks + l_p2_tot_attacks,
l_kills = l_p1_tot_kills + l_p2_tot_kills,
l_errors = l_p1_tot_errors + l_p1_tot_errors,
l_aces = l_p1_tot_aces + l_p2_tot_aces,
l_blocks = l_p1_tot_blocks + l_p2_tot_blocks,
l_serrve_errors = l_p1_tot_serve_errors + l_p2_tot_serve_errors,
l_blocks = l_p1_tot_blocks + l_p2_tot_blocks,
l_digs = l_p1_tot_digs + l_p2_tot_digs,
) %>%
na_omit()
vb_parsed <- vb_matches %>%
transmute(
circuit,
gender,
year,
w_attacks = w_p1_tot_attacks + w_p2_tot_attacks,
w_kills = w_p1_tot_kills + w_p2_tot_kills,
w_errors = w_p1_tot_errors + w_p1_tot_errors,
w_aces = w_p1_tot_aces + w_p2_tot_aces,
w_blocks = w_p1_tot_blocks + w_p2_tot_blocks,
w_serrve_errors = w_p1_tot_serve_errors + w_p2_tot_serve_errors,
w_blocks = w_p1_tot_blocks + w_p2_tot_blocks,
w_digs = w_p1_tot_digs + w_p2_tot_digs,
l_attacks = l_p1_tot_attacks + l_p2_tot_attacks,
l_kills = l_p1_tot_kills + l_p2_tot_kills,
l_errors = l_p1_tot_errors + l_p1_tot_errors,
l_aces = l_p1_tot_aces + l_p2_tot_aces,
l_blocks = l_p1_tot_blocks + l_p2_tot_blocks,
l_serrve_errors = l_p1_tot_serve_errors + l_p2_tot_serve_errors,
l_blocks = l_p1_tot_blocks + l_p2_tot_blocks,
l_digs = l_p1_tot_digs + l_p2_tot_digs,
) %>%
na.omit()
vb_parsed
vb_parsed <- vb_matches %>%
transmute(
circuit,
gender,
year,
w_attacks = w_p1_tot_attacks + w_p2_tot_attacks,
w_kills = w_p1_tot_kills + w_p2_tot_kills,
w_errors = w_p1_tot_errors + w_p1_tot_errors,
w_aces = w_p1_tot_aces + w_p2_tot_aces,
w_blocks = w_p1_tot_blocks + w_p2_tot_blocks,
w_serve_errors = w_p1_tot_serve_errors + w_p2_tot_serve_errors,
w_blocks = w_p1_tot_blocks + w_p2_tot_blocks,
w_digs = w_p1_tot_digs + w_p2_tot_digs,
l_attacks = l_p1_tot_attacks + l_p2_tot_attacks,
l_kills = l_p1_tot_kills + l_p2_tot_kills,
l_errors = l_p1_tot_errors + l_p1_tot_errors,
l_aces = l_p1_tot_aces + l_p2_tot_aces,
l_blocks = l_p1_tot_blocks + l_p2_tot_blocks,
l_serve_errors = l_p1_tot_serve_errors + l_p2_tot_serve_errors,
l_blocks = l_p1_tot_blocks + l_p2_tot_blocks,
l_digs = l_p1_tot_digs + l_p2_tot_digs,
) %>%
na.omit()
vb_parsed %>%
select(circuit, gender, year,
w_attacks:w_digs) %>%
rename_with(~ str_remove_all(., "w_"),
w_attacks:w_digs)
winners <- vb_parsed %>%
select(circuit, gender, year,
w_attacks:w_digs) %>%
rename_with(~str_remove_all(., "w_"),
w_attacks:w_digs) %>%
mutate(win = "win")
losers <- vb_parsed %>%
select(circuit, gender, year,
l_attacks:l_digs) %>%
rename_with(~str_remove_all(., "l_"),
l_attacks:l_digs) %>%
mutate(win = "lose")
vb_df <- bind_rows(
winners,
losers
) %>%
mutate_if(is.character, factor)
vb_df <- bind_rows(winners, losers) %>%
mutate_if(is.character, factor)
vb_df <- bind_rows(
vb_parsed %>%
select(circuit, gender, year,
w_attacks:w_digs) %>%
rename_with( ~ str_remove_all(., "w_"),
w_attacks:w_digs) %>%
mutate(win = "win"),
vb_parsed %>%
select(circuit, gender, year,
l_attacks:l_digs) %>%
rename_with( ~ str_remove_all(., "l_"),
l_attacks:l_digs) %>%
mutate(win = "lose")
) %>%
mutate_if(is.character, factor)
vb_df %>%
pivot_longer(attacks:digs,
names_to = "stat",
values_to = "value")
vb_df %>%
pivot_longer(attacks:digs,
names_to = "stat",
values_to = "value") %>%
ggplot(aes(x = gender,
y = value,
fill = win)) +
geom_boxplot() +
facet_wrap(~stat, scales = "free_y")
vb_df %>%
pivot_longer(attacks:digs,
names_to = "stat",
values_to = "value") %>%
ggplot(aes(x = gender,
y = value,
colour = win)) +
geom_boxplot(alpha = .4) +
facet_wrap(~stat,
scales = "free_y",
nrow = 2) +
labs(y = "",
colour = "",
fill = "")
vb_df %>%
pivot_longer(attacks:digs,
names_to = "stat",
values_to = "value") %>%
ggplot(aes(x = gender,
y = value,
colour = win,
fill = win)) +
geom_boxplot(alpha = .4) +
facet_wrap(~stat,
scales = "free_y",
nrow = 2) +
labs(y = "",
colour = "",
fill = "")
vb_split <- initial_split(vb_df, strata = win)
library(tidymodels)
vb_split <- initial_split(vb_df, strata = win)
train <- training(vb_split)
test <- testing(vb_split)
xgb_spec <- boost_tree(
trees = 1000,
tree_depth = tune(),
min_n = tune(),
loss_reduction = tune(),
sample_size = tune(),
# How much randomness is included in the model
mtry = tune(),
learn_rate = tune()
) %>%
set_engine("xgboost") %>%
set_mode("classification")
xgb_grid <- grid_latin_hypercube(
tree_depth(),
min_n(),
loss_reduction(),
sample_size = sample_prop(),
# You need to give it the training data as it doesn't know what the max number of predictors is
finalize(mtry(), vb_train),
learn_rate(),
# You usually want to go higher than 20
size = 20
)
xgb_grid <- grid_latin_hypercube(
tree_depth(),
min_n(),
loss_reduction(),
sample_size = sample_prop(),
# You need to give it the training data as it doesn't know what the max number of predictors is
finalize(mtry(), train),
learn_rate(),
# You usually want to go higher than 20
size = 20
)
xgb_wf <- workflow %>%
add_formula(win ~ .) %>%
add_model(xgb_spec)
xgb_wf <- workflow() %>%
add_formula(win ~ .) %>%
add_model(xgb_spec)
set.seed(123)
vb_folds <- vfold_cv(train, strata = win)
doParallel::registerDoParallel()
set.seed(234)
xgb_res <- tune_grid(
xgb_wf,
resamples = vb_folds,
grid = xgb_grid,
control = control_grid(save_pred = TRUE)
)
xgb_res
xgb_res
xgb_res %>%
collect_metrics()
xgb_res %>%
collect_metrics() %>%
filter(.metric == "roc_auc") %>%
select(mean, mtry:sample_size) %>%
pivot_longer(cols = mtry:sample_size,
names_to = "parameter",
values_to = "value") %>%
ggplot(aes(x = value,
y = mean,
colour = parameter)) +
geom_point(show.legend = FALSE) +
facet_wrap(~parameter, scales = "free_x")
# show_best shows the 5 best models
show_best(xgb_res, "roc_auc")
# For better results, it would be best to increase the size in the construction of the latin hypercube grid
# If not, you will be limited by "auto-generated" values
best_auc <- select_best(xgb_res, "roc_auc")
final_xgb <- finalize_workflow((wgb_wf, best_auc))
final_xgb <- finalize_workflow((xgb_wf, best_auc))
final_xgb <- finalize_workflow(xgb_wf, best_auc)
final_xgb
library(vip)
final_xgb %>%
fit(data = train) %>%
pull_workflow_fit() %>%
vip(geom = "point")
final_res <- last_fit(final_xgb, vb_split)
final_res %>% collect_metrics()
# These are predictions on the testing data
# Meaning that we can do things like confusion matrices
final_res %>%
collect_predictions() %>%
conf_mat(win, .pred_class)
final_res %>%
collect_predictions() %>%
roc_curve(win, .pred_win) %>%
autoplot()
